# Importar bibliotecas
import os
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from asserts.pdf_processor import load_pdf, chunk_text

# Instancia o modelo de incorporaÃ§Ã£o
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

persist_directory = "./chroma_db"
if not os.path.exists(persist_directory):
    os.makedirs(persist_directory)

# Instancia o vector store com o modelo de incorporaÃ§Ã£o
vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)

def index_pdf(pdf_path="./files/imposto_renda.pdf"):  
    """Indexa um documento PDF no banco de vetores"""
    print(f"ğŸ“‚ Carregando PDF do caminho: {pdf_path}")

    if not os.path.exists(pdf_path):
        print(f"âŒ Erro: O arquivo {pdf_path} nÃ£o foi encontrado.")
        return

    text = load_pdf(pdf_path)
    if not text:
        print("âŒ Erro: Nenhum texto para indexar.")
        return

    print(f"ğŸ“„ Texto extraÃ­do do PDF: {text[:500]}...")

    chunks = chunk_text(text)
    if not chunks:
        print("âŒ Erro: Nenhum chunk gerado.")
        return

    print(f"âœ‚ï¸ Total de chunks gerados: {len(chunks)}")
    print(f"ğŸ“ Exemplo de chunk: {chunks[0]}")

    print("ğŸš€ Indexando documento no banco de vetores...")
    try:
        vector_store.add_texts(chunks)
    except Exception as e:
        print(f"âŒ Erro ao adicionar textos ao banco de vetores: {e}")
        return

    indexed_count = vector_store._collection.count()
    print(f"ğŸ“š Documentos indexados no banco: {indexed_count}")

    if indexed_count == 0:
        print("âŒ Erro: Nenhum documento foi indexado! Verifique a conexÃ£o com o banco de vetores.")
    
    print("ğŸ” Verificando conteÃºdo no banco de vetores...")
    stored_texts = vector_store._collection.peek(3)  
    print(f"ğŸ“„ Textos armazenados: {stored_texts}")

def split_query(query, chunk_size=256, chunk_overlap=25):
    """Divide a query em pedaÃ§os menores para busca eficiente."""
    if len(query) > chunk_size:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        return text_splitter.split_text(query)
    return [query]

def search_documents(query, top_k=5):
    print(f"ğŸ” Buscando documentos com a query: {query}")

    indexed_count = vector_store._collection.count()
    if indexed_count == 0:
        print("âŒ Erro: Nenhum documento estÃ¡ indexado. Certifique-se de que o PDF foi processado corretamente.")
        return []

    query_chunks = split_query(query)

    try:
        query_embeddings = [embedding_model.embed_query(chunk) for chunk in query_chunks]
    except Exception as e:
        print(f"âŒ Erro ao gerar embeddings da query: {e}")
        return []

    results = []
    for embedding in query_embeddings:
        try:
            result = vector_store.similarity_search_by_vector(embedding, k=top_k)
            results.extend(result)
        except Exception as e:
            print(f"âŒ Erro na busca por similaridade: {e}")
            return []

    results = list({res.page_content: res for res in results}.values())

    if not results:
        print("âš ï¸ Nenhum resultado encontrado. Verifique a indexaÃ§Ã£o.")

    print(f"ğŸ“Œ Total de resultados encontrados: {len(results)}")
    for res in results[:3]:  
        print(f"ğŸ“„ Resultado encontrado: {res.page_content[:500]}...")

    return [res.page_content for res in results]
