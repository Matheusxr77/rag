import csv
import time
import os
import streamlit as st
from asserts.llm_interface import generate_response
from asserts.pdf_processor import load_pdf  
from asserts.vector_store import embedding_model  # Para capturar modelo de embedding
from asserts.metrics import load_questions_answers, calculate_metrics  # Importe as fun√ß√µes de m√©tricas
from asserts.config import METRICS_CSV

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# T√≠tulo e descri√ß√£o
st.title("üìÑ Chat RAG - Imposto de Renda")
st.write("Fa√ßa perguntas sobre o imposto de renda e obtenha respostas baseadas nos dados do governo federal!")

# Campo de entrada
query = st.chat_input("Digite sua pergunta:")

# Verificar se a consulta n√£o est√° vazia
if query:
    # Adicionar pergunta do usu√°rio ao hist√≥rico
    st.session_state.chat_history.append(("user", query))

    # Obter a resposta do modelo
    start_time = time.time()
    response = generate_response(query)
    response_time = time.time() - start_time

    # Adicionar resposta do modelo ao hist√≥rico
    if response:
        st.session_state.chat_history.append(("assistant", response))
        
# Exibir todo o hist√≥rico de chat
for role, response in st.session_state.chat_history:
    with st.chat_message(role):
        if role == "assistant":
            # Formatar a resposta do assistente com markdown
            st.markdown(response)
        else:
            # Mostrar a pergunta do usu√°rio
            st.markdown(response)

# Carregar perguntas e respostas esperadas
questions, expected_answers = load_questions_answers()

# Calcular m√©tricas globais constantemente
calculate_metrics(questions, expected_answers)